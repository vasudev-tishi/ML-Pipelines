{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ASSIGNMENT 4**\n",
        "### **Reinforcement Learning (Grid World)**"
      ],
      "metadata": {
        "id": "blI-fv0Z0OAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5y57CwaA0aom"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Setup**"
      ],
      "metadata": {
        "id": "wv_Lwl9y0UUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XLXE0alm0G-v"
      },
      "outputs": [],
      "source": [
        "grid_size = 5\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_size = len(actions)\n",
        "\n",
        "Q_qlearning = np.zeros((grid_size, grid_size, action_size))\n",
        "Q_sarsa = np.zeros((grid_size, grid_size, action_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Helper Functions**"
      ],
      "metadata": {
        "id": "4wOjoQoy0gj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_state(state, action):\n",
        "    x, y = state\n",
        "    if action == 0 and x > 0: x -= 1\n",
        "    if action == 1 and x < grid_size - 1: x += 1\n",
        "    if action == 2 and y > 0: y -= 1\n",
        "    if action == 3 and y < grid_size - 1: y += 1\n",
        "    return x, y\n",
        "\n",
        "def choose_action(Q, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(action_size)\n",
        "    return np.argmax(Q[state[0], state[1]])\n"
      ],
      "metadata": {
        "id": "cy7unfjH0XMh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-**Learning**"
      ],
      "metadata": {
        "id": "JKwLc2Fx0mfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "episodes = 500\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = (0, 0)\n",
        "\n",
        "    while state != (4, 4):\n",
        "        action = choose_action(Q_qlearning, state, epsilon)\n",
        "        next_state = get_next_state(state, action)\n",
        "\n",
        "        reward = 10 if next_state == (4, 4) else -1\n",
        "\n",
        "        Q_qlearning[state[0], state[1], action] += alpha * (\n",
        "            reward + gamma * np.max(Q_qlearning[next_state[0], next_state[1]])\n",
        "            - Q_qlearning[state[0], state[1], action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n"
      ],
      "metadata": {
        "id": "aFO3BZzG0keB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SARSA**"
      ],
      "metadata": {
        "id": "Npk40VOg0tRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(episodes):\n",
        "    state = (0, 0)\n",
        "    action = choose_action(Q_sarsa, state, epsilon)\n",
        "\n",
        "    while state != (4, 4):\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = 10 if next_state == (4, 4) else -1\n",
        "        next_action = choose_action(Q_sarsa, next_state, epsilon)\n",
        "\n",
        "        Q_sarsa[state[0], state[1], action] += alpha * (\n",
        "            reward + gamma * Q_sarsa[next_state[0], next_state[1], next_action]\n",
        "            - Q_sarsa[state[0], state[1], action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "        action = next_action\n"
      ],
      "metadata": {
        "id": "M02eFDjZ0rUE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Policy Comparison**"
      ],
      "metadata": {
        "id": "jCM8CSBt0zac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-Learning Policy (max action per state):\")\n",
        "print(np.argmax(Q_qlearning, axis=2))\n",
        "\n",
        "print(\"\\nSARSA Policy (max action per state):\")\n",
        "print(np.argmax(Q_sarsa, axis=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLabn3fs0wL5",
        "outputId": "ab734c0e-bca4-4267-b42d-c4779c8cc58a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning Policy (max action per state):\n",
            "[[1 1 1 3 3]\n",
            " [3 1 1 1 1]\n",
            " [3 1 1 1 1]\n",
            " [3 3 1 1 1]\n",
            " [3 3 3 3 0]]\n",
            "\n",
            "SARSA Policy (max action per state):\n",
            "[[1 1 3 1 1]\n",
            " [3 1 1 1 1]\n",
            " [3 1 1 1 1]\n",
            " [3 3 3 1 1]\n",
            " [3 3 3 3 0]]\n"
          ]
        }
      ]
    }
  ]
}